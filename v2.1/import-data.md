---
title: Import Data
summary: Learn how to import data into a CockroachDB cluster.
toc: false
---

CockroachDB supports importing data from CSV/TSV or SQL dump files.

{{site.data.alerts.callout_info}}To import/restore data from CockroachDB-generated <a href="backup.html">enterprise license backups</a>, see <a href="restore.html"><code>RESTORE</code></a>.{{site.data.alerts.end}}

<div id="toc"></div>

## Import from Tabular Data (CSV)

If you have data exported in a tabular format (e.g., CSV or TSV), you can use the [`IMPORT`](import.html) statement.

To use this statement, though, you must also have some kind of remote file server (such as Amazon S3 or a custom file server) that all your nodes can access.

## Import from Generic SQL Dump

You can execute batches of `INSERT` statements stored in `.sql` files (including those generated by [`cockroach dump`](sql-dump.html)) from the command line, importing data into your cluster.

{% include copy-clipboard.html %}
~~~ shell
$ cockroach sql --database=[database name] < statements.sql
~~~

{{site.data.alerts.callout_success}}Grouping each <code>INSERT</code> statement to include approximately 500-10,000 rows will provide the best performance. The number of rows depends on row size, column families, number of indexes; smaller rows and less complex schemas can benefit from larger groups of <code>INSERTS</code>, while larger rows and more complex schemas benefit from smaller groups.{{site.data.alerts.end}}

## Import from PostgreSQL Dump

If you're importing data from a PostgreSQL deployment, you can import the `.sql` file generated by the `pg_dump` command to more quickly import data.

{{site.data.alerts.callout_success}}The <code>.sql</code> files generated by <code>pg_dump</code> provide better performance because they use the <code>COPY</code> statement instead of bulk <code>INSERT</code> statements.{{site.data.alerts.end}}

### Create PostgreSQL SQL File

Which `pg_dump` command you want to use depends on whether you want to import your entire database or only specific tables:

- Entire database:

    {% include copy-clipboard.html %}
    ~~~ shell
    $ pg_dump [database] > [filename].sql
    ~~~

- Specific tables:

    {% include copy-clipboard.html %}
    ~~~ shell
    $ pg_dump -t [table] [table's schema] > [filename].sql
    ~~~

For more details, see PostgreSQL's documentation on [`pg_dump`](https://www.postgresql.org/docs/9.1/static/app-pgdump.html).

### Reformat SQL File

After generating the `.sql` file, you need to perform a few editing steps before importing it:

1. Remove all statements from the file besides the `CREATE TABLE` and `COPY` statements.
2. Manually add the table's [`PRIMARY KEY`](primary-key.html#syntax) constraint to the `CREATE TABLE` statement.
  This has to be done manually because PostgreSQL attempts to add the primary key after creating the table, but CockroachDB requires the primary key be defined upon table creation.
3. Review any other [constraints](constraints.html) to ensure they're properly listed on the table.
4. Remove any [unsupported elements](sql-feature-support.html).

### Import Data

After reformatting the file, you can import it through `psql`:

{% include copy-clipboard.html %}
~~~ shell
$ psql -p [port] -h [node host] -d [database] -U [user] < [file name].sql
~~~

For reference, CockroachDB uses these defaults:

- `[port]`: **26257**
- `[user]`: **root**


## Import from MySQL Dump

This section has instructions for getting data out of MySQL and into CockroachDB.

- [Step 1. Dump the MySQL database](#step-1-dump-the-mysql-database)
- [Step 2. Convert the database to CSV](#step-2-convert-the-database-to-csv)
- [Step 3. IMPORT the CSV](#step-3-import-the-csv)

### Step 1. Dump the MySQL database

{% include copy-clipboard.html %}
~~~ sh
$ mysqldump -u root test --tab ./cockroach-data/extern --fields-enclosed-by='"' --fields-escaped-by='\'
~~~

Note that your MySQL user will need permission to execute [`SELECT INTO OUTFILE`](https://dev.mysql.com/doc/refman/8.0/en/select.html).  This is required by [`mysqldump --tab`](https://dev.mysql.com/doc/refman/8.0/en/mysqldump-delimited-text.html).

Always specify an escape character when running `mysqldump --tab`.  If your data happens to include any of the special characters, the dumped data cannot be decoded correctly if they are not escaped.  We recommend also specifying an enclose character.

### Step 2. Convert the database to CSV

{% include copy-clipboard.html %}
~~~ sh
$ ./mysqlout-to-csv --chunk-rows 100000 --fields-enclosed-by='"' --fields-escaped-by='\' cockroach-data/extern/tblATS_job.txt cockroach-data/extern/tbl.csv
~~~

Note that [`IMPORT`](import.html) may mishandle non-Unicode CSV content and recommends hex-encoding binary fields.  The `mysqlout-to-csv` command can hex-encode the fields you specify via the `--hex-cols` flag.

Note that chunking your CSVs such that you have at least as many CSV files as nodes allows utilizing all nodes, since IMPORT distributes work by assigning each file to a node.

### Step 3. IMPORT the CSV

The command below shows how to run [`IMPORT`](import.html) from the command line using [the built-in SQL client](use-the-built-in-sql-client.html).  You can also issue the [`IMPORT`](import.html) statement directly at the SQL prompt.

In either case, you will need to replace the IP address, database name, and other information shown below to match the environment you're working with.

{% include copy-clipboard.html %}
~~~ sh
cockroach sql --insecure --host 104.196.49.87 -d dbname -e "IMPORT TABLE \"table1\" CREATE USING 's3://nate-external-storage/schema.sql?AWS_ACCESS_KEY_ID=ACCESSKEY&AWS_SECRET_ACCESS_KEY=SECRET' CSV DATA ('s3://nate-external-storage/tbl.csv.1?AWS_ACCESS_KEY_ID=ACCESSKEY&AWS_SECRET_ACCESS_KEY=SECRET', 's3://nate-external-storage/tbl.csv.2?AWS_ACCESS_KEY_ID=ACCESSKEY&AWS_SECRET_ACCESS_KEY=SECRET') WITH nullif='\N'"
~~~

Note that:

- If you create multiple chunks, you need to point to each chunk in remote storage by name
- If you do not want to use remote storage, you can either:
  - Copy all files to each node's `extern` subdirectory, or
  - (Recommended) Start up an [off-the-shelf HTTP server](create-a-file-server.html#using-caddy-as-a-file-server) to serve the files from where they are

## See Also

- [SQL Dump (Export)](sql-dump.html)
- [Back up Data](back-up-data.html)
- [Restore Data](restore-data.html)
- [Use the Built-in SQL Client](use-the-built-in-sql-client.html)
- [Other Cockroach Commands](cockroach-commands.html)
- [`IMPORT`](import.html)
